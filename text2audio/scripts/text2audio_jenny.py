# ç¼ºç‚¹ï¼Œéœ€è¦è¿æ¥huggingface
import os
# å½»åº•ç¦ç”¨æ‰€æœ‰ç¼–è¯‘ä¼˜åŒ–
os.environ["TORCHINDUCTOR_DISABLE"] = "1"
os.environ["UNSLOTH_DISABLE_COMPILATION"] = "1"
os.environ["TORCHDYNAMO_DISABLE"] = "1"
os.environ["UNSLOTH_OFFLINE"] = "1"  # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["TRITON_CACHE_DIR"] = ""  # æ¸…ç©ºtritonç¼“å­˜

import torch
# å½»åº•ç¦ç”¨æ‰€æœ‰åŠ¨æ€ç¼–è¯‘
torch._dynamo.config.suppress_errors = True

# åœ¨å¯¼å…¥unslothä¹‹å‰è®¾ç½®
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

from unsloth import FastLanguageModel
from snac import SNAC
import scipy.io.wavfile
import numpy as np

print("å¼€å§‹åŠ è½½æ¨¡å‹...")

# åŠ è½½æ¨¡å‹ - ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=r"G:\AIModels\modelscope_cache\models\Vyvo\VyvoTTS-LFM2-Jenny",
    max_seq_length=8192,
    dtype=None,
    load_in_4bit=False,
    # æ·»åŠ æ›´å¤šä¿å®ˆè®¾ç½®
    # trust_remote_code=True,
    # use_safetensors=True,
    local_files_only=True  # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
)

print("TTSæ¨¡å‹åŠ è½½å®Œæˆ")

# å½»åº•ç¦ç”¨æ¨¡å‹å†…éƒ¨çš„æ‰€æœ‰ä¼˜åŒ–
if hasattr(model, 'config'):
    model.config.use_cache = True
    model.config.torch_dtype = torch.float32

# ç¦ç”¨ä»»ä½•å¯èƒ½çš„æ¢¯åº¦æ£€æŸ¥ç‚¹
if hasattr(model, 'gradient_checkpointing_disable'):
    model.gradient_checkpointing_disable()
    
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

print("åŠ è½½SNACå£°ç å™¨...")
# å°è¯•ä»æœ¬åœ°åŠ è½½SNACæ¨¡å‹
try:
    # é¦–å…ˆå°è¯•é»˜è®¤è·¯å¾„
    snac_model = SNAC.from_pretrained(r"G:\AIModels\hf_cache\snac_24khz", local_files_only=True)
except:
    try:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•å¸¸è§ç¼“å­˜è·¯å¾„
        snac_path = os.path.expanduser("~/.cache/huggingface/hub/models--hubertsiuzdak--snac_24khz")
        snac_model = SNAC.from_pretrained(snac_path, local_files_only=True)
    except:
        print("âŒ æ— æ³•æ‰¾åˆ°SNACæ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹")
        print("è¿è¡Œ: huggingface-cli download hubertsiuzdak/snac_24khz --local-dir ./snac_24khz")
        exit(1)

print("SNACå£°ç å™¨åŠ è½½å®Œæˆ")

print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å¤„ç†æ–‡æœ¬...")

tokeniser_length = 64400

# å®šä¹‰token
start_of_text = 1
end_of_text = 7
start_of_speech = tokeniser_length + 1
end_of_speech = tokeniser_length + 2
start_of_human = tokeniser_length + 3
end_of_human = tokeniser_length + 4
pad_token = tokeniser_length + 7
audio_tokens_start = tokeniser_length + 10

# è¾“å…¥æ–‡æœ¬
# prompts = ["Hey there my name is Elise, and I'm a speech generation model that can sound like a person."]
prompts = ["Hi, My name is haibin, and I'm a speech generation model that can sound like a person."]
chosen_voice = None

# å‡†å¤‡æ¨ç† - ä¸ä½¿ç”¨FastLanguageModelçš„ä¼˜åŒ–
# FastLanguageModel.for_inference(model)  # æ³¨é‡Šæ‰è¿™ä¸€è¡Œï¼Œä½¿ç”¨æ ‡å‡†æ–¹å¼

prompts_ = [(f"{chosen_voice}: " + p) if chosen_voice else p for p in prompts]

all_input_ids = []
for prompt in prompts_:
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    all_input_ids.append(input_ids)

start_token = torch.tensor([[start_of_human]], dtype=torch.int64)
end_tokens = torch.tensor([[end_of_text, end_of_human]], dtype=torch.int64)

all_modified_input_ids = []
for input_ids in all_input_ids:
    modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1)
    all_modified_input_ids.append(modified_input_ids)

# å¡«å……åºåˆ—
all_padded_tensors, all_attention_masks = [], []
max_length = max([m.shape[1] for m in all_modified_input_ids])
for m in all_modified_input_ids:
    padding = max_length - m.shape[1]
    padded_tensor = torch.cat([torch.full((1, padding), pad_token, dtype=torch.int64), m], dim=1)
    attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, m.shape[1]), dtype=torch.int64)], dim=1)
    all_padded_tensors.append(padded_tensor)
    all_attention_masks.append(attention_mask)

input_ids = torch.cat(all_padded_tensors, dim=0).to("cuda")
attention_mask = torch.cat(all_attention_masks, dim=0).to("cuda")

print("å¼€å§‹ç”ŸæˆéŸ³é¢‘token...")

# ä½¿ç”¨æœ€ä¿å®ˆçš„ç”Ÿæˆæ–¹å¼ï¼Œä¸ä½¿ç”¨ä»»ä½•autocast
with torch.no_grad():
    # å®Œå…¨ç¦ç”¨ä»»ä½•å½¢å¼çš„ç¼–è¯‘
    torch._dynamo.disable()
    
    generated_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=800,  # å‡å°‘tokenæ•°é‡ä»¥é™ä½å†…å­˜ä½¿ç”¨
        do_sample=True,
        temperature=0.6,
        top_p=0.95,
        repetition_penalty=1.1,
        num_return_sequences=1,
        eos_token_id=end_of_speech,
        use_cache=True,
        pad_token_id=pad_token
    )

print("éŸ³é¢‘tokenç”Ÿæˆå®Œæˆï¼Œå¼€å§‹å¤„ç†...")

# å¤„ç†ç”Ÿæˆçš„token
token_to_find = start_of_speech
token_to_remove = end_of_speech
token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)

if len(token_indices[1]) > 0:
    last_occurrence_idx = token_indices[1][-1].item()
    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]
else:
    cropped_tensor = generated_ids

processed_rows = []
for row in cropped_tensor:
    masked_row = row[row != token_to_remove]
    processed_rows.append(masked_row)

code_lists = []
for row in processed_rows:
    row_length = row.size(0)
    new_length = (row_length // 7) * 7
    trimmed_row = row[:new_length]
    trimmed_row = [t - audio_tokens_start for t in trimmed_row]
    code_lists.append(trimmed_row)

def redistribute_codes(code_list):
    layer_1, layer_2, layer_3 = [], [], []
    for i in range((len(code_list)+1)//7):
        layer_1.append(code_list[7*i])
        layer_2.append(code_list[7*i+1]-4096)
        layer_3.append(code_list[7*i+2]-(2*4096))
        layer_3.append(code_list[7*i+3]-(3*4096))
        layer_2.append(code_list[7*i+4]-(4*4096))
        layer_3.append(code_list[7*i+5]-(5*4096))
        layer_3.append(code_list[7*i+6]-(6*4096))
    codes = [
        torch.tensor(layer_1).unsqueeze(0),
        torch.tensor(layer_2).unsqueeze(0),
        torch.tensor(layer_3).unsqueeze(0)
    ]
    audio_hat = snac_model.decode(codes)
    return audio_hat

print("å¼€å§‹è§£ç éŸ³é¢‘...")

# ç”ŸæˆéŸ³é¢‘
my_samples = []
for i, code_list in enumerate(code_lists):
    print(f"è§£ç éŸ³é¢‘ {i+1}/{len(code_lists)}...")
    samples = redistribute_codes(code_list)
    my_samples.append(samples)

# ä¿å­˜éŸ³é¢‘æ–‡ä»¶
print("\n=== éŸ³é¢‘ç”Ÿæˆç»“æœ ===")
for i, (prompt, samples) in enumerate(zip(prompts, my_samples)):
    print(f"æç¤º: {prompt}")
    
    # ä¿å­˜ä¸ºWAVæ–‡ä»¶
    output_file = f"generated_audio_{i}.wav"
    audio_data = samples.detach().squeeze().to("cpu").numpy()
    
    # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æ­£ç¡®æ ¼å¼
    if audio_data.dtype != np.int16:
        # å½’ä¸€åŒ–å¹¶è½¬æ¢ä¸º16ä½PCM
        audio_data = np.int16(audio_data / np.max(np.abs(audio_data)) * 32767)
    
    scipy.io.wavfile.write(output_file, rate=24000, data=audio_data)
    print(f"âœ… éŸ³é¢‘å·²ä¿å­˜: {output_file}")
    
    # æ˜¾ç¤ºéŸ³é¢‘ä¿¡æ¯
    duration = len(audio_data) / 24000
    print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
    print("-" * 50)

print("\nğŸ‰ æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
print("æ‚¨å¯ä»¥åœ¨å½“å‰ç›®å½•æ‰¾åˆ°ç”Ÿæˆçš„WAVæ–‡ä»¶")

# å½»åº•æ¸…ç†å†…å­˜
del my_samples, model, snac_model, tokenizer
torch.cuda.empty_cache()

print("å†…å­˜æ¸…ç†å®Œæˆ")